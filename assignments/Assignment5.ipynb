{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS 575: Assignment 5\n",
    "\n",
    "- Turn in solutions as a single notebook (ipynb) _and_ as a pdf on Blackboard. No need to turn in datasets/word-docs.\n",
    "- Answer the following questions concisely, in complete sentences and with full clarity. Across group collaboration is _strictly_ not allowed. Always cite all your sources.\n",
    "- Make appropriate assumptions necessary in order to answer the questions, and mention them explicitly at the beginning of each answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive-Bayes Text Categorization (5pt)\n",
    "\n",
    "\n",
    "In this problem, you are to work on a text classification problem by using the same data provided for Problem 3 in Assignment 4. Your goal is to classify each text article into one of the $4$ categories by using a multi-class Naive-Bayes model rather than using multi-class SVMs. If you use a Bernoulli Naive-Bayes model, the class label for a document that consists of n different word types is given by the following expressions:\n",
    "\n",
    "$$ p(y = k|x_1, ..., x_n) = \\frac{p(x_1, ..., x_n|y = k)p(y = k)}{p(x_1, ..., x_n)}$$\n",
    "$$\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\propto  p(y = k)p(x_1,...,x_n|y = k)$$\n",
    "$$\\quad\\quad\\quad\\quad\\quad\\quad\\quad = p(y = k)\\Pi_{j=1}^{n}p(x_j|y = k).$$\n",
    "\n",
    "It means that if a document contains the $j$-th word ($x_j$) at least once (among its $n$ unique words out of $|V|$ vocabulary), then that information can be used to classify the documents class label $y$ as $k$ by finding the $\\arg\\max_{k\\in \\{1,2,3,4\\}} p(y = k|x_1, ..., x_n)$.\n",
    "\n",
    "In the above, the most probable class label given your word observations is based only on the appearance of each word ignoring how many times they appear. Recall that the denominator $p(x_1, ..., x_n)$ in the equations above does not affect on the class prediction because it does not contain the $y = k$ related term. So, it can be removed as shown. The last equality in the above equations is a result of the Naive-Bayes assumption. \n",
    "\n",
    "On the other hand, if you use a multinomial Naive-Bayes model, then the document's class label can be decided by:\n",
    "\n",
    "$$ p(y = k|x_1, ..., x_n) = \\frac{p(x_1, ..., x_n|y = k)p(y = k)}{p(x_1, ..., x_n)}$$\n",
    "$$\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\propto  p(y = k)p(x_1,...,x_n|y = k)$$\n",
    "$$\\quad\\quad\\quad\\quad\\quad\\quad\\quad = p(y = k)\\Pi_{j=1}^{n}p(x_j|y = k)^{l_j},$$\n",
    "\n",
    "where $l_j$ is the number of times word $j$ appears. This means that our prediction takes into account not only of the existence of each word, but also of its frequency. Due to the Naive-Bayes assumption, our conditional probability $p(x_j|y = k)$ for $j$-th word is now multiplied $l_j$ times as every occurrence is conditionally independent given the class label $y = k$. In contrast, when we use Bernoulli Naive-Bayes, we are treating all appearing words equally with the corresponding $l_j := 1$.\n",
    "\n",
    "Try to load the training and test data. While each article can be seen as a vector in $mathbb{R}^{|V|}$, you should only store each of the existing words and their counts and ignore non-existing words. Note that the fact that a word $x_j$ has appeared $l_j$ times is recorded in our dataset as a sequence of $j : l_j$ terms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The category of each text article must depend on the meaning of its content. Explain why Naive-Bayes assumption is not too unrealistic for text categorization problem. (Hint: In the equations above, the product (denoted by $\\Pi$) is invariant even if you multipliy $p(x_j|y = k)$ in various different orders).\n",
    "\n",
    "2. Train the Bernoulli Naive-Bayes model based on the training data using MLE. For each class $k$ among 4 different classes, you should learn the parameter $\\phi_{j|y=k}$, which is the conditional probability $p(x_j|y = k)$. You should also learn the parameter $\\phi_{y=k}$, which is the prior probability of each class $p(y = k)$. Report the confusion matrix and training accuracy by predicting the class labels of the training set by your trained Bernoulli Naive-Bayes model.\n",
    "\n",
    "3. Learn the model parameters again by performing Laplace smoothing. Report the new confusion matrix and training accuracy when predicting on the training data. Report another confusion matrix and test accuracy when predicting on the test data.\n",
    "\n",
    "4. Redo parts 2 and 3 with the multinomial Naive-Bayes model.\n",
    "\n",
    "5. Compare and contrast the results from parts 3 and 4. Justify why one works better than the other in our dataset. Explain, in general, the weakness of these Naive-Bayes models by comparing the Bernoulli and the multinomial variants. (Hint: Think about what happen if the same word occurs multiple times in an article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hidden Markov Models (3pt)\n",
    "\n",
    "Suppose we have a Hidden Markov Model (HMM) with 3 hidden states $(S_1,S_2,S_3)$ and the corresponding observations $(O_1,O_2,O_3)$. Each state can take $k$ different values, and there are a total $m$ observations across all states.\n",
    "\n",
    "1. Count the number of parameters to define the initial distribution, the transition distribution, and the emission distribution.\n",
    "\n",
    "2. Does the number of parameters depend on the number of states? Briefly justify your answer.\n",
    "\n",
    "3. Enumerate all conditional independence holding in this HMM.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The specific initial, transition, and emission distributions are given for the following problems. Answer the following questions by using these probability values. These are not coding questions!\n",
    "\n",
    "\n",
    "| $\\textrm{State}$| $p(S_1)$ |\n",
    "|-------|-------|\n",
    "|  A &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 0.99 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n",
    "|  B | 0.01  |\n",
    "\n",
    "| $S_t$| $S_{t+1}$  | $p(S_{t+1}|S_{t})$   |\n",
    "|---|---|---|\n",
    "|  A &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | A  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 0.99 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n",
    "|  A | B  | 0.01  |\n",
    "|  B | A  |  0.01 |\n",
    "|  B | B  |  0.99 |\n",
    "\n",
    "\n",
    "| $S_t$| $O_{t}$  | $p(O_{t}|S_{t})$   |\n",
    "|---|---|---|\n",
    "|  A &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 0  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 0.8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n",
    "|  A | 1  | 0.2  |\n",
    "|  B | 0  |  0.1 |\n",
    "|  B | 1  |  0.9 |\n",
    "\n",
    "4. Compute the probability of observing the sequence $(O_1, O_2, O_3) = (0, 1, 0)$ when using the forward algorithm. Try to include both the intermediate equations of values and the final value after computation.\n",
    "\n",
    "5. Compute the probability of observing the sequence $(O_1, O_2, O_3) = (0, 1, 0)$ when using the backward algorithm. Include the intermediate evaluations similarly to part 4.\n",
    "\n",
    "6. Are the two results from the forward and backward algorithms same? What is the most likely sequence of values for these three states? Note that each state takes a value of either A or B. Include all intermediate evaluations that lead you to your conclusion. (Hint: You can reuse your work in part 4 and 5).\n",
    "\n",
    "7. If you use Viterbi algorithm, what is the most likely sequence of values for these three states? Include all intermediate evaluations that lead you to your conclusion.\n",
    "\n",
    "8. Try to find the most likely sequence of values for considering each state separately (i.e., by ignoring between-state dependency). Compare whether the resulting sequence of values is equivalent to what you have observed in part 6 and 7. Justify why and whether this holds in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-means Clustering (1.5pt)\n",
    "\n",
    "Given the following $6$ data points, we will perform K-means clustering with K = 2 below (both manually/step-by-step and automatically). Each example consists of two features and initially assigned to the cluster $k$.\n",
    "\n",
    "| $i$| $x_{1}$  | $x_{2}$   | $k$  |\n",
    "|---|---|---|---|\n",
    "|  1 | 1  | 4  | 1  |\n",
    "|  2 | 1  | 3  | 2  |\n",
    "|  3 | 0  |  4 | 1  |\n",
    "|  4 | 5  |  1 | 1  |\n",
    "|  5 |  6 |  2 | 1  |\n",
    "|  6 |  4 |  0 | 2  |\n",
    "\n",
    "\n",
    "1. Manually:\n",
    " - Plot the data points, and compute the centroid of each cluster.\n",
    " - Assign each data point to its closest centroid, reporting the new cluster label $k$.\n",
    " - Repeat until convergence. Once the centroids and the cluster labels stop changing, report the cluster label $k$ for each data point.\n",
    "\n",
    "2. Automatically:\n",
    " - Implement K-means from scratch.\n",
    " - Check whether the final clustering given by K-means matches your the clustering result obtained in part 1. \n",
    " - Is the clustering result always same regardeless of the initial cluster assignment for this dataset? In general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Principal Component Analysis and K-means (2.5pt)\n",
    "\n",
    "For this problem, first generate a synthetic data set with 20 data points for each of the three classes. Each data point must consist of 50 features, so that we have total $60 \\times 50$ data matrix. Feel free to use the [numpy random number generation functions](https://numpy.org/devdocs/reference/random/legacy.html). Ensure that you add a mean shift to the data points in order to make them appear as three distinctive classes. In this question you are going to play with two unsupervised algorithms: PCA and K-means clustering.\n",
    "\n",
    "1. Run PCA on these 60 data points. Plot the first two principle axes. Try to use different colors to contrast the data points that belong to different classes. If the three classes are distinctive enough, continue to the next part. Otherwise keep synthesizing a new dataset until you reach at some degree of separations across three classes in terms of the two principal component axes.\n",
    "\n",
    "2. Run K-means clustering with K = 3. Compare the obtained clusters to the true class labels. (Hint: Note that K-means clustering only separates without assigning particular label values. Use [normalized mutual information](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html) to compare.)\n",
    "\n",
    "3. Run K-means clustering with K = 2 and 4. Explain your results in contrast to part 2.\n",
    "\n",
    "4. Now run K-means clustering with K = 3 only on the two principal axes that you discovered in part 1. In other words, run K -means clustering on $60 \\times 2$ matrix. Explain your result in comparison to the previous results.\n",
    "\n",
    "5. Implement standardizing from scratch (make each feature have a standard deviation of 1, and do not remove the existing mean shift in each feature). Given the original $60\\times 50$ data that you worked on part 2, run K-means clustering with K = 3 after standardizing each feature. Explain your result in comparison to the results of part 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
