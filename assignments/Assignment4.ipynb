{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS 575: Assignment 4\n",
    "\n",
    "- Turn in solutions as a single notebook (ipynb) _and_ as a pdf on Blackboard. No need to turn in datasets/word-docs.\n",
    "- Answer the following questions concisely, in complete sentences and with full clarity. Across group collaboration is _strictly_ not allowed. Always cite all your sources.\n",
    "- Make appropriate assumptions necessary in order to answer the questions, and mention them explicitly at the beginning of each answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generalized Linear Model (2pt)\n",
    "\n",
    "Generalized Linear Models provide a standardized recipe for developing a new learning model and algorithm. In our lecture, you have seen three examples: Linear regression, Logistic regression, and Multi-class classification where outputs given an input are modeled by Normal, Bernoulli, and Categorical distributions, respectively. In this problem, your output will be a different type of random variable, which models the number of times that an event occurs in an interval of time or space. The distribution you are to use has the following form:\n",
    "$$p(y; \\lambda) = \\frac{e^{−\\lambda}\\lambda^{y}}{y!},$$\n",
    "\n",
    "where the only parameter $\\lambda$ indicates the rate of the event which occurs independently. For instance, f you receive in average 4 postal mails every day, then receiving any particular mail will not affect the arrival times of future mails. For this example, $\\lambda$ = 4.\n",
    "\n",
    "1. Prove or disprove whether this distribution is in the exponential family. If it is an expo- nential family, write down clearly what are the natural parameter $\\eta$, the natural sufficient statistics $T(\\eta)$, the log partition function $a(\\eta)$, and the base measure $b(y)$.\n",
    "\n",
    "2. Recall that the natural parameter of Logistic Regression was the log odd ratio of Bernoulli parameter (e.g., $\\eta = \\log\\frac{1-\\phi}{\\phi} $). When we express $\\phi$ in terms of $\\eta$, it corresponded to the logistic function, explaining why the logistic function is the natural choice for binary classification. What will be such a natural function for our distribution? Try to express $\\eta$ in terms of $\\lambda$. (Hints: Mean of this distribution is $\\lambda$)\n",
    "\n",
    "3. Suppose you are given a training data $D = \\{(x^{(i)},y^{(i)})| 1 \\leq i \\leq m\\}$. Evaluate the partial derivative of $p(y^{(i)}|x^{(i)};\\theta)$ with respect to $\\theta_j$. Derive a stochastic gradient ascent algorithm for learning the model parameter $\\theta$.\n",
    "\n",
    "4. In class we have seen both Linear regression and Logistic regression have exactly same learning algorithms, and it was explained because they are all GLMs. For a majority of members of the exponential family that satisfy $T(y) = y$, provide a reason why the stochastic gradient algorithm on its log-likelihood shares the same update $\\theta_j := \\theta_j −\\alpha(h_{\\theta}(x)−y)x_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Binary SVM Model (2pt)\n",
    "\n",
    "Given the following toy dataset consisting of seven exmaples just two features, you are supposed to learn a binary SVM classifier from [scikit-learn](https://scikit-learn.org/stable/modules/svm.html) (in particular the SVC model with parameter kernel='linear').\n",
    "\n",
    "\n",
    "| i   | $x_{1}$  | $x_{2}$   | y  |\n",
    "|---|---|---|---|\n",
    "|  1 | 3  | 4  | y  |\n",
    "|  2 | 2  | 2  | y  |\n",
    "|  3 | 4  |  4 | y  |\n",
    "|  4 | 1  |  4 | y  |\n",
    "|  5 |  2 |  1 | n  |\n",
    "|  6 |  4 |  1 | n  |\n",
    "|  7 |  4 |  3 | n  |\n",
    "\n",
    "1. Visualize all seven training examples and sketch the decision boundary produced by the SVM classifier (again, this is from [scikit-learn](https://scikit-learn.org/stable/modules/svm.html)). Write down all  relevant model settings chosen. There is no need for test/validation splitting for this.\n",
    "\n",
    "2. Is the data linearly separable? \n",
    "\n",
    "3. Write down the equation for this decision boundary. Also, state the rule of classification between 'y' and 'n'. It should be of the form 'y' if $\\theta_1x_1 + \\theta_2x_2 + \\theta_0 > 0$, and 'n' otherwise.\n",
    "\n",
    "4. Would a slight perturbation of the 6-th example affect the SVM solution? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multiclass Support Vector Machine (8pt)\n",
    "\n",
    "Text classification is an important problem in information retrieval and natural language processing. The task is to classify each text article into one of the predefined classes/categories. Here you have four sets of articles about: 1) operating systems, 2) vehicles, 3) sports, and 4) politics. Each set consists of various articles collected from a corresponding newsgroup where each article is represented by Bag-of-Words (BoW) features. That is, after extracting all the relevant words from the entire dataset, feature vectors are defined as the frequency of words occurring in each article.\n",
    "\n",
    "Since you have more than two classes, using a single binary SVM classifier is not sufficient. Instead, you are going to combine several binary SVM classifiers to predict multiple classes. Thus, the goal is to train multiple SVM classifiers that predict binary classes based on BoW features, then combining them to properly predict one of the four classes. You can download the following data files from the [dropbox folder](https://www.dropbox.com/sh/5inrhmbf4h6xpw4/AAAwer0s6ilzoGghOKCwAE8qa/data?dl=0&subfolder_nav_tracking=1).\n",
    "\n",
    " - *words.map*: A word table mapping every relevant word into its line number. In other words, this is a simple text file having one word per each line. Every word corresponds to its unique line number starting from 1.\n",
    " - *articles.train*: Training data consisting of 4,000 articles (1,000 articles per class) \n",
    " - *articles.test*: Test data consisting of 2,400 articles (600 articles per class)\n",
    "\n",
    "A single line in the training and test data has the following format:\n",
    " - Format: (Class #) (Features) \n",
    " - Class #: One of 1, 2, 3, 4 (1=operating systems, 2=vehicles, 3=sports, 4=politics) \n",
    " - Features: A space-separated sequence of word #:frequency\n",
    "\n",
    "Example: If a line is 1 11:2 13:1 23:12 25:1 27:2 28:1, for example, this is an article about operating system which contains word 11 (addresses) twice, word 13 (organizations) once, and so forth. Recall that the word-integer mapping information is given in the words.map where each word is mapped into its line number.\n",
    "\n",
    "1. Load the training and test data into pandas data frames. Justify your dataframe schema choices.\n",
    "\n",
    "2. First, train four different linear SVM classifiers (i.e., SVC model with parameter kernel=’linear’). As SVM classifies only binary labels, you have to replace the target class number to $1$ and all others to $-1$ before calling the fit function (see [svm doc page](https://scikit-learn.org/stable/modules/svm.html)). For instance, if you try to classify whether or not politics, you are to use 1,000 articles about politics as positive samples and 3,000 others as negative samples for training. Once you learn the four classifiers, the output label of each test example x is determined by the following formula:\n",
    "\n",
    "$$h_{w,b}(x) = \\arg\\max_{k \\in \\{1,2,3,4\\}}(w(k)^T x + b(k)),$$\n",
    "\n",
    "where $w(k)$ is the learned weight vector, and $b(k)$ is the biased term for the class $k$. If the prediction $h_{w,b}(x)$ is different from the ground-truth class, it yields an error. Report the training and test errors of the individual four classifiers as well as for $h_{w,b}(x)$.\n",
    "\n",
    "3. Now you are to train linear SVM classifiers with different $C$ values (see [the parameter description here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)) from $\\{0.125, 0.25, 0.5, 1, 2, 4, 8, ... , 256, 512\\}$. In order to pick the best C value, you are required to perform a hold-out validation (write the below logic from scratch):\n",
    "\n",
    "  1. Split the entire training data randomly into 75% for training and 25% for validation. Do not use scikit-learn or other external package for this, but write it from scratch.\n",
    "  2. For each $C$ value, learn four binary classifiers similar to part 2 but only on the training data.\n",
    "  3. Measure the overall classification error on the validation data.\n",
    "  4. Pick the $C$ with the lowest validation error.\n",
    "\n",
    "Plot a graph showing both training and validation errors together with varying C in log-scale. (i.e., x-axis: $log_2 C$, y-axis: error rate) What are the best C values for multiclass classification? (Note: You should apply one uniform $C$ value identically to all four SVC classifiers).\n",
    "\n",
    "4. With the best $C$ value chosen from part 3, learn four linear SVC classifiers again on the entire training set. (Note that your classifiers from part 3 used only 75% of the training set for learning, holding out 25% for validation) Test your newly learned best classifiers on the test set similar to part 2 where the output label is determined by the argmax class given in the formula in part 2. Compare the test error rates to the classifiers on part 2. Which classifier works better? Justify your observation.\n",
    "\n",
    "5. For this problem, you will normalize feature vectors so that the feature vectors of each example have unit length. For each example $x = (x_1, x_2, ..., x_n)$, divide every component by $\\lVert x\\rVert_2$ so that $\\lVert x\\rVert_2 = 1$. Repeat parts 3 and 4 with normalized features and measure the test error rates again with newly picked C value. Compare the new test error to previous test error of classifier from part 4 without normalization, and explain why normalization makes a difference (if any).\n",
    "\n",
    "6. What you have done so far is one way to extend binary SVM to a multiclass classifier, which is called 1-vs-all. There is another way of getting a multiclass classifier out of binary classifiers called 1-vs-1, where you train all $ {4 \\choose 2}$ binary classifiers distinguishing between every pair of 2\n",
    "classes. Then for each test example, you will pick the (not necessarily unique) class that achieves the highest votes.\n",
    "Formally speaking, suppose that the binary classifier $h_{ij}$ is trained by taking the examples from class $i$ as positives and the examples from class $j$ as negatives. Then for each test example x, add one vote to class $i$ if $h_{ij}$ says x is in class $i$. Otherwise, add one vote to class $j$. Finally 1-vs-1 assigns $x$ to the class with the maximum votes. Since vote is added by one each time, the maximum voted class is not necessarily unique.\n",
    "Compare the accuracy of 1-vs-1 to 1-vs-all. Accuracy should be measured fairly between two methods via using the normalized features as done the in part 5 and the best C parameters as done in the part 3.\n",
    "\n",
    "7. Compare and contrast your results from part 6 with the corresponding implementations found in [scikit-learn](https://scikit-learn.org/stable/modules/svm.html#multi-class-classification).\n",
    "\n",
    "8. Implement k-fold cross validation procedure from scratch. Use this to determine $C$ in part 3 and 4 and the resulting training and test errors. Comment on the numbers and the choice of $C$ comuted here when compared to the hold-out validation approach in part 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
