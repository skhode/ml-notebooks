{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS 575: Assignment 3\n",
    "\n",
    "- Turn in solutions as a single notebook (ipynb) _and_ as a pdf on Blackboard. No need to turn in datasets/word-docs.\n",
    "- Answer the following questions concisely, in complete sentences and with full clarity. Across group collaboration is _strictly_ not allowed. Always cite all your sources.\n",
    "- Make appropriate assumptions necessary in order to answer the questions, and mention them explicitly at the beginning of each answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Probabilistic Perspective of Linear Regression (1pt)\n",
    "\n",
    "In addition to the least square objective, we also learned the probabilistic perspective for linear regression, where each observation is assumed to have a Gaussian noise. (i.e., Noise of each example is an independent and identically distributed sample from a normal distribution). Lets assume that we are given the following regression model that includes two linear features and one quadratic feature.\n",
    "$$y=\\theta_0+\\theta_1 x_1+\\theta_2 x_2+\\theta_3 x_1^2+\\epsilon \\textrm{ where } \\epsilon\\sim N(0,\\sigma^2)$$\n",
    "Your goal is to relate the likelihood maximization objective to the least squares objective.\n",
    "\n",
    " 1. Is $y$ linear with respect to $\\theta$? Is $y$ linear with respect to $\\{x_1,x_2\\}$?\n",
    " 2. Given the definition of noise, derive the corresponding mean and variance parameters of the normal distribution for $y|x_1, x_2; \\theta$. Also write down its probability density function.\n",
    " 3. You are provided with a training observations $D = \\{x_1^{(i)},x_2^{(i)},y^{(i)}\\} $ where $i = 1,...,m$. Derive the conditional log-likelihood that will be later maximized to make the dataset $D$ most likley.\n",
    " 4. If you omit all the constants that do not relate to our parameters $\\theta$, what will the objective function $J(\\{\\theta_0, \\theta_1, \\theta_2, \\theta_3\\})$ (to perform Maximum Likelihood Estimation) look like? Does $J$ look similar to the Least Square objective for this problem? What are the differences, if any?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression Toy Problem (1pt)\n",
    "\n",
    "Recall the grading problem to predict pass/fail in the class. Suppose you collect data for a group of students in the class that consist of two input features $X_1$ = hours studied and $X_2$ = undergrad GPA. Your goal is to predict the output $Y \\in \\{pass, fail\\}$. Suppose that you fit a logistic regression, learning its parameter $(\\theta_0, \\theta_1, \\theta_2) = (âˆ’6, 0.05, 1)$.\n",
    "\n",
    " 1. Define a python function, which given inputs $X_1,X_2,\\theta$ will return the probability of passing the class.\n",
    " 2. What will be the probability of passing the class for a student who studies for 40 hours and has a GPA of 3.5?\n",
    " 3. How many hours would the aforementioned student need to study in order to have at least 50% chance of passing the class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Logistic Regression (6pt)\n",
    "\n",
    "The following questions must be answered using the [Weekly dataset](https://www.dropbox.com/sh/5inrhmbf4h6xpw4/AAAwer0s6ilzoGghOKCwAE8qa/data?dl=0&preview=Weekly.csv&subfolder_nav_tracking=1) (originally found in the [ISLR package in R](https://rdrr.io/cran/ISLR/man/Weekly.html)). This dataset contains 1,089 weekly returns for 21 years from the beginning of 1990 to the end of 2010. You will use its 1990-2008 as a training data and 2009-2010 as a test data to perform logistic regression.\n",
    "\n",
    " 1. In the data, the input features are five of *Lag* variables and *Volume*, and the binary output is *Direction*. Read the csv into a pandas dataframe and describe the relevant features and target variables of the dataset.\n",
    " 2. We plan to reuse the SGD procedure from Assignment 2 to fit a logistic regression model. Do we need to write a new gradient function tailored to binary logistic regression, or can we use the one written in Assignment 2?\n",
    " 3. Extend the gradient function such that a regularized logistic loss is being minimized. In particular, consider an $\\ell_2$ penalty on the parameters (except the intercept). For this, the gradient function will need: (a) a penalty input that can take two values: \"none\" and \"l2\", and (b) an optional regularization coefficient $C$ input. Summarize the changes from the previously written gradient function that were necessary to accomplish this.\n",
    " 4. Write a python function that takes the predicted labels and the true label arrays, and computes the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    " 5. Use the above function to report the confusion matrix and the accuracy on both training and test data for two models:\n",
    "   1. the learned model when the regularization is absent, and \n",
    "   2. the learned model when regularization is present. \n",
    "   *(Hint: As nothing is specified, your default threshold to decide Up/Down can be 0.5.)* \n",
    " 6. Also plot the two losses (training and test) as a function of epoch for each model. \n",
    "   1. Are we overfitting in comparison to the test loss? Why/why not? \n",
    "   2. Is the model without the regularization overfitting? \n",
    "   3. Is the model with regualization underfitting?\n",
    " 7. Compare the performance of these two models with a corresponding model fit using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    " 8. How do the various parameters (learning rate, regularization coefficient) influence our results?\n",
    " 9. Implement a python function that takes the predicted labels and the true label arrays, and computes the [F-score](https://en.wikipedia.org/wiki/F-score).\n",
    " 9. Now you will run logistic regression (without regularization, and using our SGD procedure) five times with only one input features *Lagj* ($j=1,...,5$) for each time. Compute the confusion matrix, F score (using our implementations) and the accuracy on both training and test data given each of the learned models. \n",
    "   1. Which are the best models among the five models here and the earlier models in part 5, in terms of the accuracy and F-score, respectively? \n",
    "   2. Does the best model also achieve the best accuracy or F-score on the training data?\n",
    "   *(Hint: The best model must be chosen based on the test data, not the training data!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ROC Curves (4pt)\n",
    "\n",
    "1. Write a python function that takes the predicted labels and the true label arrays, and computes the [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve as well as the area under the ROC curve/Area Under Curve(AUC). \n",
    "2. Use the above function and matplotlib to draw ROC curves for the models from Question 3 with varying thresholds. Determine the best model in terms of the AUC. (Note: all ROC curves must be plotted in a single plot). \n",
    "3. Repeat step 2 with the reference implementation from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html).\n",
    "4. Fit suitable k-nn and decision tree models (using your implementations from Assignment 1) to the Weekly dataset in Question 2, and compare them with the best logistic regression model obtained in part 2. Report the confusion matrices, accuracies, AUCs and the ROC curves (on the same plot) and comment on which model is the better one among the three types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
